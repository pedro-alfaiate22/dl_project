{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras import layers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import os, shutil"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdc5434b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def get_true_pred(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in dataset.unbatch().batch(1):\n",
    "        y_true.append(np.argmax(labels.numpy()))\n",
    "        pred = model.predict(images, verbose=0)\n",
    "        y_pred.append(np.argmax(pred))\n",
    "    return np.array(y_true), np.array(y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e745654ed31ac85",
   "metadata": {},
   "source": [
    "train_dir = 'Dataset/archive/seg_train'\n",
    "validation_dir = 'Dataset/archive/seg_val'\n",
    "test_dir = 'Dataset/archive/seg_test'\n",
    "\n",
    "train_buildings_dir = 'Dataset/archive/seg_train/buildings/'\n",
    "train_forest_dir = 'Dataset/archive/seg_train/forest'\n",
    "train_glacier_dir = 'Dataset/archive/seg_train/glacier'\n",
    "train_mountain_dir = 'Dataset/archive/seg_train/mountain'\n",
    "train_sea_dir = 'Dataset/archive/seg_train/sea'\n",
    "train_street_dir = 'Dataset/archive/seg_train/street'\n",
    "\n",
    "val_buildings_dir = 'Dataset/archive/seg_val/buildings'\n",
    "val_forest_dir = 'Dataset/archive/seg_val/forest'\n",
    "val_glacier_dir = 'Dataset/archive/seg_val/glacier'\n",
    "val_mountain_dir = 'Dataset/archive/seg_val/mountain'\n",
    "val_sea_dir = 'Dataset/archive/seg_val/sea'\n",
    "val_street_dir = 'Dataset/archive/seg_val/street'\n",
    "\n",
    "test_buildings_dir = 'Dataset/archive/seg_test/buildings'\n",
    "test_forest_dir = 'Dataset/archive/seg_test/forest'\n",
    "test_glacier_dir = 'Dataset/archive/seg_test/glacier'\n",
    "test_mountain_dir = 'Dataset/archive/seg_test/mountain'\n",
    "test_sea_dir = 'Dataset/archive/seg_test/sea'\n",
    "test_street_dir = 'Dataset/archive/seg_test/street'\n",
    "\n",
    "print('total training buildings images:', len(os.listdir(train_buildings_dir)))\n",
    "print('total training forest images:', len(os.listdir(train_forest_dir)))\n",
    "print('total training glacier images:', len(os.listdir(train_glacier_dir)))\n",
    "print('total training mountain images:', len(os.listdir(train_mountain_dir)))\n",
    "print('total training sea images:', len(os.listdir(train_sea_dir)))\n",
    "print('total training street images:', len(os.listdir(train_street_dir)))\n",
    "\n",
    "print('total validation buildings images:', len(os.listdir(val_buildings_dir)))\n",
    "print('total validation forest images:', len(os.listdir(val_forest_dir)))\n",
    "print('total validation glacier images:', len(os.listdir(val_glacier_dir)))\n",
    "print('total validation mountain images:', len(os.listdir(val_mountain_dir)))\n",
    "print('total validation sea images:', len(os.listdir(val_sea_dir)))\n",
    "print('total validation street images:', len(os.listdir(val_street_dir)))\n",
    "\n",
    "print('total test buildings images:', len(os.listdir(test_buildings_dir)))\n",
    "print('total test forest images:', len(os.listdir(test_forest_dir)))\n",
    "print('total test glacier images:', len(os.listdir(test_glacier_dir)))\n",
    "print('total test mountain images:', len(os.listdir(test_mountain_dir)))\n",
    "print('total test sea images:', len(os.listdir(test_sea_dir)))\n",
    "print('total test street images:', len(os.listdir(test_street_dir)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b695c6ac4f576af1",
   "metadata": {},
   "source": [
    "IMG_SIZE = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Processing the data\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "print(test_dataset)\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Classes:\", class_names)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffb8aca2e5b94ba4",
   "metadata": {},
   "source": [
    "model = keras.models.load_model('modelT_featureExtraction_full.keras')\n",
    "model_DA = keras.models.load_model('modelT_featureExtraction_DataAugmentation.keras')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc3351ace57d458f",
   "metadata": {},
   "source": [
    "convbase = model.get_layer(\"vgg16\")\n",
    "convbase.trainable = True\n",
    "for layer in convbase.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(convbase.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37cd35646f737d05",
   "metadata": {},
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fa899595f706c34",
   "metadata": {},
   "source": [
    "checkpoint_filepath = 'modelT_fineTuning.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "136f903e57d98d04",
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "train_dataset,\n",
    "epochs=50,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=[model_checkpoint_callback])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "381c5de5d995db63",
   "metadata": {},
   "source": [
    "# Loading and testing the model\n",
    "model = keras.models.load_model('modelT_fineTuning.keras')\n",
    "val_loss, val_acc = model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e9d8dbe16ef26d",
   "metadata": {},
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a190d99f2127fc4",
   "metadata": {},
   "source": [
    "convbase_DA = model_DA.get_layer(\"vgg16\")\n",
    "convbase_DA.trainable = True\n",
    "for layer in convbase_DA.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(convbase_DA.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee6b88c876b2f7f4",
   "metadata": {},
   "source": [
    "model_DA.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96839cc386940961",
   "metadata": {},
   "source": [
    "checkpoint_filepath_DA = 'modelT_fineTuning_DataAugmentation.keras'\n",
    "model_checkpoint_callback_DA = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_DA,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41b224f1942f6982",
   "metadata": {},
   "source": [
    "history_DA = model_DA.fit(\n",
    "train_dataset,\n",
    "epochs=50,   \n",
    "validation_data=validation_dataset,\n",
    "callbacks=[model_checkpoint_callback_DA])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c700b56ff10b6c3",
   "metadata": {},
   "source": [
    "model_DA = keras.models.load_model('modelT_fineTuning_DataAugmentation.keras')\n",
    "val_loss, val_acc = model_DA.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e60a45ff1192fa6",
   "metadata": {},
   "source": [
    "acc = history_DA.history['acc']\n",
    "val_acc = history_DA.history['val_acc']\n",
    "loss = history_DA.history['loss']\n",
    "val_loss = history_DA.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a767d8e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "y_true, y_pred = get_true_pred(model_DA, test_dataset)\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "class_only_report = {k: v for k, v in report.items() if k in class_names}\n",
    "df = pd.DataFrame(class_only_report).T\n",
    "print(df[['precision', 'recall', 'f1-score']].round(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdd6b28e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "val_loss_, val_acc = model.evaluate(validation_dataset)\n",
    "val_loss_DA, val_acc_CatCross_DA= model_DA.evaluate(validation_dataset)\n",
    "\n",
    "print(\"Validation Accuracy dos modelos:\")\n",
    "print(f\"Fine tuning: {val_loss_:.4f}\")\n",
    "print(f\"Fine tuning + Data augmentation: {val_loss_DA:.4f}\")\n",
    "\n",
    "\n",
    "results = {\n",
    "    'FineTuning': val_acc,\n",
    "    'FineTuning_DataAugmentation': val_loss_DA\n",
    "}\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "best_accuracy = results[best_model]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model} com val_accuracy = {best_accuracy:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd625eae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "img_path = 'Dataset/archive/seg_test/sea/20072.jpg'\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    img_path,\n",
    "    target_size=(150, 150),\n",
    "    interpolation='bilinear'\n",
    ")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagem de Teste\")\n",
    "plt.show()\n",
    "\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "# Previsão\n",
    "result = model.predict(img_array)\n",
    "\n",
    "class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "print(\"Probabilidades por classe:\")\n",
    "for i, prob in enumerate(result[0]):\n",
    "    print(f\"{class_names[i]:>10s}: {prob:.4f}\")\n",
    "\n",
    "# Classe prevista\n",
    "predicted_class = np.argmax(result)\n",
    "print(f\"\\nClasse prevista: {class_names[predicted_class]} ({result[0][predicted_class]:.4f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

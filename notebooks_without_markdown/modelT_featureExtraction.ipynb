{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras import layers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, shutil"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "650400f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def get_true_pred(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in dataset.unbatch().batch(1):\n",
    "        y_true.append(np.argmax(labels.numpy()))\n",
    "        pred = model.predict(images, verbose=0)\n",
    "        y_pred.append(np.argmax(pred))\n",
    "    return np.array(y_true), np.array(y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b20ef9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85c6f9454db56efe",
   "metadata": {},
   "source": [
    "train_dir = 'Dataset/archive/seg_train'\n",
    "validation_dir = 'Dataset/archive/seg_val'\n",
    "test_dir = 'Dataset/archive/seg_test'\n",
    "\n",
    "train_buildings_dir = 'Dataset/archive/seg_train/buildings/'\n",
    "train_forest_dir = 'Dataset/archive/seg_train/forest'\n",
    "train_glacier_dir = 'Dataset/archive/seg_train/glacier'\n",
    "train_mountain_dir = 'Dataset/archive/seg_train/mountain'\n",
    "train_sea_dir = 'Dataset/archive/seg_train/sea'\n",
    "train_street_dir = 'Dataset/archive/seg_train/street'\n",
    "\n",
    "val_buildings_dir = 'Dataset/archive/seg_val/buildings'\n",
    "val_forest_dir = 'Dataset/archive/seg_val/forest'\n",
    "val_glacier_dir = 'Dataset/archive/seg_val/glacier'\n",
    "val_mountain_dir = 'Dataset/archive/seg_val/mountain'\n",
    "val_sea_dir = 'Dataset/archive/seg_val/sea'\n",
    "val_street_dir = 'Dataset/archive/seg_val/street'\n",
    "\n",
    "test_buildings_dir = 'Dataset/archive/seg_test/buildings'\n",
    "test_forest_dir = 'Dataset/archive/seg_test/forest'\n",
    "test_glacier_dir = 'Dataset/archive/seg_test/glacier'\n",
    "test_mountain_dir = 'Dataset/archive/seg_test/mountain'\n",
    "test_sea_dir = 'Dataset/archive/seg_test/sea'\n",
    "test_street_dir = 'Dataset/archive/seg_test/street'\n",
    "\n",
    "print('total training buildings images:', len(os.listdir(train_buildings_dir)))\n",
    "print('total training forest images:', len(os.listdir(train_forest_dir)))\n",
    "print('total training glacier images:', len(os.listdir(train_glacier_dir)))\n",
    "print('total training mountain images:', len(os.listdir(train_mountain_dir)))\n",
    "print('total training sea images:', len(os.listdir(train_sea_dir)))\n",
    "print('total training street images:', len(os.listdir(train_street_dir)))\n",
    "\n",
    "print('total validation buildings images:', len(os.listdir(val_buildings_dir)))\n",
    "print('total validation forest images:', len(os.listdir(val_forest_dir)))\n",
    "print('total validation glacier images:', len(os.listdir(val_glacier_dir)))\n",
    "print('total validation mountain images:', len(os.listdir(val_mountain_dir)))\n",
    "print('total validation sea images:', len(os.listdir(val_sea_dir)))\n",
    "print('total validation street images:', len(os.listdir(val_street_dir)))\n",
    "\n",
    "print('total test buildings images:', len(os.listdir(test_buildings_dir)))\n",
    "print('total test forest images:', len(os.listdir(test_forest_dir)))\n",
    "print('total test glacier images:', len(os.listdir(test_glacier_dir)))\n",
    "print('total test mountain images:', len(os.listdir(test_mountain_dir)))\n",
    "print('total test sea images:', len(os.listdir(test_sea_dir)))\n",
    "print('total test street images:', len(os.listdir(test_street_dir)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e2fd18f1b4dc19f",
   "metadata": {},
   "source": [
    "IMG_SIZE = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Processing the data\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "print(test_dataset)\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Classes:\", class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e08bb5efccbf623e",
   "metadata": {},
   "source": [
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d28c073809a6289",
   "metadata": {},
   "source": [
    "train_features, train_labels = get_features_and_labels(train_dataset)\n",
    "val_features, val_labels = get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels = get_features_and_labels(test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c7aaa12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "np.save('modelT_train_features.npy', train_features)\n",
    "np.save('modelT_train_labels.npy', train_labels)\n",
    "np.save('modelT_val_features.npy', val_features)\n",
    "np.save('modelT_val_labels.npy', val_labels)\n",
    "np.save('modelT_test_features.npy', test_features)\n",
    "np.save('modelT_test_labels.npy', test_labels)\n",
    "\n",
    "train_features = np.load('modelT_train_features.npy')\n",
    "train_labels = np.load('modelT_train_labels.npy')\n",
    "val_features = np.load('modelT_val_features.npy')\n",
    "val_labels = np.load('modelT_val_labels.npy')\n",
    "test_features = np.load('modelT_test_features.npy')\n",
    "test_labels = np.load('modelT_test_labels.npy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74d089aba0984b24",
   "metadata": {},
   "source": [
    "inputs = keras.Input(shape=(4, 4, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "563324b1bab2c7f3",
   "metadata": {},
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86ebc27bf49a20cf",
   "metadata": {},
   "source": [
    "checkpoint_filepath = 'modelT_featureExtraction_head.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0d0e9d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "print('Is eager: ', tf.executing_eagerly())\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=50,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=[model_checkpoint_callback])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a1f5cf2f0e17a11",
   "metadata": {},
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cbeec1109a5f52e",
   "metadata": {},
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = keras.applications.vgg16.preprocess_input(inputs)\n",
    "x = conv_base(x)\n",
    "outputs = model(x)\n",
    "full_model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a068348bd5b5ff2",
   "metadata": {},
   "source": [
    "full_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54c99d872404e61e",
   "metadata": {},
   "source": [
    "full_model.save('modelT_featureExtraction_full.keras')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcf543ffeeccc87e",
   "metadata": {},
   "source": [
    "full_model = keras.models.load_model('modelT_featureExtraction_full.keras')\n",
    "val_loss, val_acc = full_model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be33f9a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from PIL import Image\n",
    "img_path = 'Dataset/archive/seg_test/sea/20072.jpg'\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    img_path,\n",
    "    target_size=(150, 150),\n",
    "    interpolation='bilinear'\n",
    ")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagem de Teste\")\n",
    "plt.show()\n",
    "\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "result = full_model.predict(img_array)\n",
    "\n",
    "class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "print(\"Probabilidades por classe:\")\n",
    "for i, prob in enumerate(result[0]):\n",
    "    print(f\"{class_names[i]:>10s}: {prob:.4f}\")\n",
    "\n",
    "predicted_class = np.argmax(result)\n",
    "print(f\"\\nClasse prevista: {class_names[predicted_class]} ({result[0][predicted_class]:.4f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79d6b9fa32ab88aa",
   "metadata": {},
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "     layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f48b0167091d743",
   "metadata": {},
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "x = conv_base_DA(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model_DA = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64cb897adf9b35b5",
   "metadata": {},
   "source": [
    "model_DA.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d611354a30f2d3b1",
   "metadata": {},
   "source": [
    "checkpoint_filepath_DA = 'modelT_featureExtraction_DataAugmentation.keras'\n",
    "model_checkpoint_callback_DA = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_DA,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc1f8171e34ec9ab",
   "metadata": {},
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "print('Is eager: ', tf.executing_eagerly())\n",
    "history_DA = model_DA.fit(\n",
    "train_dataset,\n",
    "epochs=50,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=[model_checkpoint_callback_DA])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acd3353d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "print(f\"Melhor época (menor val_loss): {best_epoch}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e56e90dbf68240ed",
   "metadata": {},
   "source": [
    "model_DA = keras.models.load_model('modelT_featureExtraction_DataAugmentation.keras')\n",
    "val_loss, val_acc = model_DA.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1117b5ae8143f8d",
   "metadata": {},
   "source": [
    "acc = history_DA.history['accuracy']\n",
    "val_acc = history_DA.history['val_accuracy']\n",
    "loss = history_DA.history['loss']\n",
    "val_loss = history_DA.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af025b98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "img_path = 'Dataset/archive/seg_test/sea/20072.jpg'\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    img_path,\n",
    "    target_size=(150, 150),\n",
    "    interpolation='bilinear'\n",
    ")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagem de Teste\")\n",
    "plt.show()\n",
    "\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "result = model_DA.predict(img_array)\n",
    "\n",
    "class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "print(\"Probabilidades por classe:\")\n",
    "for i, prob in enumerate(result[0]):\n",
    "    print(f\"{class_names[i]:>10s}: {prob:.4f}\")\n",
    "\n",
    "predicted_class = np.argmax(result)\n",
    "print(f\"\\nClasse prevista: {class_names[predicted_class]} ({result[0][predicted_class]:.4f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c256684",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "val_loss, val_acc = full_model.evaluate(validation_dataset)\n",
    "val_loss_t_DA, val_acc_DA = model_DA.evaluate(validation_dataset)\n",
    "\n",
    "\n",
    "print(\"Validation Accuracy dos modelos:\")\n",
    "print(f\"Model feature extraction: {val_acc:.4f}\")\n",
    "print(f\"Model feature extraction + Data augmentation: {val_acc_DA:.4f}\")\n",
    "\n",
    "\n",
    "results = {\n",
    "    'Without data augmentation': val_acc,\n",
    "    'With data augmentation': val_acc_DA,\n",
    "}\n",
    "\n",
    "# Identificar o melhor modelo com base na maior val_accuracy\n",
    "best_model = max(results, key=results.get)\n",
    "best_accuracy = results[best_model]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model} com val_accuracy = {best_accuracy:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fad3c4e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "img_path = 'Dataset/archive/seg_test/sea/20072.jpg'\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    img_path,\n",
    "    target_size=(150, 150),\n",
    "    interpolation='bilinear'\n",
    ")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagem de Teste\")\n",
    "plt.show()\n",
    "\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "result = model_DA.predict(img_array)\n",
    "\n",
    "class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "print(\"Probabilidades por classe:\")\n",
    "for i, prob in enumerate(result[0]):\n",
    "    print(f\"{class_names[i]:>10s}: {prob:.4f}\")\n",
    "\n",
    "\n",
    "predicted_class = np.argmax(result)\n",
    "print(f\"\\nClasse prevista: {class_names[predicted_class]} ({result[0][predicted_class]:.4f})\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
